version: "3.10"

# Configure modules
x-airflow-common:
  &airflow-common
  image: apache/airflow:2.7.0-python3.10
  environment:
    &airflow-common-env
    S3_ENDPOINT: ${S3_ENDPOINT}
    S3_ACCESS_KEY: ${S3_ACCESS_KEY}
    S3_SECRET_KEY: ${S3_SECRET_KEY}
    S3_PATH_STYLE_ACCESS: "true"
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
    AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: false
    AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: America/Sao_Paulo
    AIRFLOW_UID: '50000'
    AIRFLOW__CORE__DEFAULT_TIMEZONE: America/Sao_Paulo
    _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_WWW_USER_USERNAME}
    _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}
    _PIP_ADDITIONAL_REQUIREMENTS: apache-airflow==2.7.0
  volumes:
    - ./src/airflow/dags:/opt/airflow/dags
    - ./src/airflow/config:/opt/airflow/config
    - ./src/airflow/plugins:/opt/airflow/plugins
    - ./src/airflow/logs:/opt/airflow/logs
  user: "50000"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

# Configure services
services:

  # Storages

  postgres:
    container_name: postgres
    hostname: postgres
    image: postgres:11
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./src/postgres/init-database.sh:/docker-entrypoint-initdb.d/init-database.sh
      - ./src/volume/postgres:/var/lib/postgresql/data

    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "postgres" ]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  minio:
    container_name: minio
    hostname: minio
    image: 'minio/minio'
    ports:
      - '9000:9000'
      - '9001:9001'
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
      MINIO_DOMAIN: ${MINIO_DOMAIN}
    command: server /data --console-address ":9001"

  minio-job:
    image: 'minio/mc'
    container_name: minio-job
    hostname: minio-job
    env_file:
      - .env
    entrypoint: |
      /bin/bash -c "
      sleep 5;
      /usr/bin/mc config --quiet host add myminio http://minio:9000 \$S3_ACCESS_KEY \$S3_SECRET_KEY || true;
      /usr/bin/mc mb --quiet myminio/datalake || true;
      "
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      - minio

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    build:
      dockerfile: ./src/hive-metastore/Dockerfile
    image: dataincode/openlakehouse:hive-metastore-3.1.2
    env_file:
      - .env
    ports:
      - '9083:9083' # Metastore Thrift
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: ${HIVE_METASTORE_JDBC_URL}
      HIVE_METASTORE_USER: ${HIVE_METASTORE_USER}
      HIVE_METASTORE_PASSWORD: ${HIVE_METASTORE_PASSWORD}
      HIVE_METASTORE_WAREHOUSE_DIR: ${HIVE_METASTORE_WAREHOUSE_DIR}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      postgres:
        condition: service_healthy

  # Trino

  trino:
    profiles: [ "trino" ]
    container_name: trino
    hostname: trino
    image: "trinodb/trino:425"
    restart: always
    ports:
      - "8889:8889"
    volumes:
      - ./src/trino/etc-coordinator:/etc/trino
      - ./src/trino/catalog:/etc/trino/catalog
    depends_on:
      - hive-metastore

  trino-worker:
    profiles: [ "trino-worker" ]
    container_name: trino-worker
    hostname: trino-worker
    image: "trinodb/trino:425"
    restart: always
    volumes:
      - ./src/trino/etc-worker:/etc/trino
      - ./src/trino/catalog:/etc/trino/catalog
    depends_on:
      - trino

  # spark-iceberg

  spark-iceberg:
    profiles: [ "spark-iceberg" ]
    build:
      dockerfile: ./src/spark/Dockerfile-spark3.4
    image: dataincode/openlakehouse:spark-3.4
    container_name: spark-iceberg
    hostname: spark-iceberg
    entrypoint: |
      /bin/bash -c "
      jupyter lab --notebook-dir=/opt/notebook --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port=8888 --no-browser --allow-root
      "
    ports:
      - "4040:4040"
      - "8900:8888"
    depends_on:
      - minio
      - hive-metastore
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_PATH_STYLE_ACCESS: "true"

    volumes:
      - ./notebook:/opt/notebook
      - ./src/jupyter/jupyter_server_config.py:/root/.jupyter/jupyter_server_config.py
      - ./src/jupyter/themes.jupyterlab-settings:/root/.jupyter/lab/user-settings/@jupyterlab/apputils-extension/themes.jupyterlab-settings
      - ./src/spark/spark-defaults-iceberg.conf:/opt/spark/conf/spark-defaults.conf

  # Airflow
  airflow-webserver:
    <<: *airflow-common
    profiles: [ "airflow", "trino", "spark-iceberg" ]
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    profiles: [ "airflow", "trino", "spark-iceberg" ]
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8974/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    profiles: [ "airflow", "trino", "spark-iceberg" ]
    container_name: airflow-worker
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  # airflow-triggerer:
  #   <<: *airflow-common
  #   command: triggerer
  #   healthcheck:
  #     test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   restart: always
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     airflow-init:
  #       condition: service_completed_successfully

  airflow-cli:
    <<: *airflow-common
    container_name: airflow-cli
    profiles: [ "airflow-debug" ]
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"

    command:
      - bash
      - -c
      - airflow

  airflow-init:
    <<: *airflow-common
    profiles: [ "airflow" ]
    container_name: airflow-init
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        mkdir -p /sources/src/airflow/logs /sources/src/airflow/dags /sources/src/airflow/plugins /sources/src/airflow/config
        echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0" > .env
        chown -R "${AIRFLOW_UID}:0" /sources/src/airflow/{logs,dags,plugins,config}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources/src/airflow

  redis:
    profiles: [ "airflow" ]
    container_name: redis
    image: redis:latest
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

# Configure Network
networks:
  default:
    name: openlakehouse
